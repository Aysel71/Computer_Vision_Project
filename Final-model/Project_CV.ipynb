{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from diffusers.pipelines.controlnet import StableDiffusionControlNetInpaintPipeline\n",
    "from diffusers import ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation, AutoModelForDepthEstimation\n",
    "from colors import ade_palette\n",
    "from utils import map_colors_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.float16"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def exclude_items(color_set, item_set, exclude_set):\n",
    "    valid_colors = []\n",
    "    valid_items = []\n",
    "    for color, item in zip(color_set, item_set):\n",
    "        if item not in exclude_set:\n",
    "            valid_colors.append(color)\n",
    "            valid_items.append(item)\n",
    "    return valid_colors, valid_items"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def setup_segmentation_pipeline():\n",
    "    processor = AutoImageProcessor.from_pretrained(\"models/openmmlab--upernet-convnext-small\")\n",
    "    segmentor = UperNetForSemanticSegmentation.from_pretrained(\"models/openmmlab--upernet-convnext-small\")\n",
    "    return processor, segmentor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_segmentation(image, processor, segmentor):\n",
    "    processed_input = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    with torch.no_grad():\n",
    "        results = segmentor(processed_input)\n",
    "\n",
    "    segmentation = processor.post_process_semantic_segmentation(\n",
    "        results, target_sizes=[image.size[::-1]]\n",
    "    )[0]\n",
    "    color_map = np.zeros((segmentation.shape[0], segmentation.shape[1], 3), dtype=np.uint8)\n",
    "    color_palette = np.array(ade_palette())\n",
    "    for label, color in enumerate(color_palette):\n",
    "        color_map[segmentation == label, :] = color\n",
    "    return Image.fromarray(color_map.astype(np.uint8)).convert('RGB')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def setup_depth_pipeline():\n",
    "    extractor = AutoImageProcessor.from_pretrained(\"models/models--LiheYoung--depth-anything-large-hf\", torch_dtype=torch.float16)\n",
    "    estimator = AutoModelForDepthEstimation.from_pretrained(\"models/models--LiheYoung--depth-anything-large-hf\", torch_dtype=torch.float16)\n",
    "    return extractor, estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def compute_depth(image, extractor, estimator):\n",
    "    input_data = extractor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        depth_output = estimator(**input_data).predicted_depth\n",
    "\n",
    "    width, height = image.size\n",
    "    depth_output = torch.nn.functional.interpolate(\n",
    "        depth_output.unsqueeze(1).float(),\n",
    "        size=(height, width),\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    depth_min = torch.amin(depth_output, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_max = torch.amax(depth_output, dim=[1, 2, 3], keepdim=True)\n",
    "    normalized_depth = (depth_output - depth_min) / (depth_max - depth_min)\n",
    "    depth_image = Image.fromarray((normalized_depth[0][0].cpu().numpy() * 255).astype(np.uint8))\n",
    "    return depth_image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def resize_to_target(dim, max_size):\n",
    "    w, h = dim\n",
    "    if max(w, h) <= max_size:\n",
    "        return dim\n",
    "    ratio = h / w if w > h else w / h\n",
    "    return (max_size, int(max_size * ratio)) if w > h else (int(max_size * ratio), max_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "class ControlNetMultiPipeline:\n",
    "    def __init__(self):\n",
    "        os.environ['HF_HUB_OFFLINE'] = \"True\"\n",
    "\n",
    "        depth_model = ControlNetModel.from_pretrained(\"models/controlnet_depth\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "        segment_model = ControlNetModel.from_pretrained(\"models/own_controlnet\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "\n",
    "        self.pipeline = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "            \"models/Realistic_Vision_V5.1_noVAE\",\n",
    "            controlnet=[depth_model, segment_model],\n",
    "            safety_checker=None,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        self.pipeline.load_ip_adapter(\"models/models--h94--IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n",
    "        self.pipeline.set_ip_adapter_scale(0.4)\n",
    "        self.pipeline.scheduler = UniPCMultistepScheduler.from_config(self.pipeline.scheduler.config)\n",
    "        self.pipeline = self.pipeline.to(device)\n",
    "\n",
    "        self.guide_pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "            \"models/models--segmind--SSD-1B\", torch_dtype=dtype, use_safetensors=True, variant=\"fp16\"\n",
    "        ).to(device)\n",
    "\n",
    "        self.seed = 323 * 111\n",
    "        self.negative_prompt = \"window, door, low resolution, banner, logo, watermark, text, deformed, blurry, out of focus, surreal, ugly, beginner\"\n",
    "        self.excluded_items = [\"windowpane;window\", \"door;double;door\"]\n",
    "        self.quality_suffix = \"interior design, 4K, high resolution, photorealistic\"\n",
    "        self.random_gen = torch.Generator(device=device).manual_seed(self.seed)\n",
    "\n",
    "        self.seg_processor, self.seg_model = setup_segmentation_pipeline()\n",
    "        self.depth_extractor, self.depth_model = setup_depth_pipeline()\n",
    "        self.depth_model = self.depth_model.to(device)\n",
    "\n",
    "    def generate_design(self, room_image, text_prompt):\n",
    "        print(text_prompt)\n",
    "        prompt = f\"{text_prompt}, {self.quality_suffix}\"\n",
    "        original_width, original_height = room_image.size\n",
    "        resized_width, resized_height = resize_to_target(room_image.size, 768)\n",
    "        resized_image = room_image.resize((resized_width, resized_height))\n",
    "\n",
    "        segmentation_map = np.array(generate_segmentation(resized_image, self.seg_processor, self.seg_model))\n",
    "        unique_colors = [tuple(color) for color in np.unique(segmentation_map.reshape(-1, 3), axis=0)]\n",
    "        color_list, item_list = exclude_items(unique_colors, [map_colors_rgb(c) for c in unique_colors], self.excluded_items)\n",
    "\n",
    "        mask = np.zeros_like(segmentation_map)\n",
    "        for color in color_list:\n",
    "            mask[(segmentation_map == color).all(axis=2)] = 1\n",
    "\n",
    "        mask_image = Image.fromarray((mask * 255).astype(np.uint8)).convert(\"RGB\")\n",
    "        depth_image = compute_depth(room_image, self.depth_extractor, self.depth_model)\n",
    "\n",
    "        ip_adapter_image = self.guide_pipeline(\n",
    "            prompt,\n",
    "            negative_prompt=self.negative_prompt,\n",
    "            height=resized_height // 8 * 8,\n",
    "            width=resized_width // 8 * 8,\n",
    "            generator=self.random_gen\n",
    "        ).images[0]\n",
    "\n",
    "        generated_image = self.pipeline(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=self.negative_prompt,\n",
    "            num_inference_steps=50,\n",
    "            strength=0.9,\n",
    "            guidance_scale=10,\n",
    "            generator=self.random_gen,\n",
    "            image=room_image,\n",
    "            mask_image=mask_image,\n",
    "            ip_adapter_image=ip_adapter_image,\n",
    "            control_image=[depth_image, Image.fromarray(segmentation_map).convert(\"RGB\")],\n",
    "            controlnet_conditioning_scale=[0.5, 0.5]\n",
    "        ).images[0]\n",
    "\n",
    "        return generated_image.resize((original_width, original_height), Image.Resampling.LANCZOS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "UserModel = ControlNetMultiPipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def process_single_image(image_path, description, pipeline_model):\n",
    "    input_image = Image.open(image_path)\n",
    "    result_image = pipeline_model.generate_design(input_image, description)\n",
    "    result_image = result_image.convert('RGB')\n",
    "    assert input_image.size == result_image.size, \"Output image dimensions must match the input image dimensions\"\n",
    "    return result_image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def execute_pipeline(prompts_filepath, dataset_dir, output_dir):\n",
    "    pipeline_model = UserModel()\n",
    "    prompts_df = pd.read_csv(prompts_filepath, sep='\\t')\n",
    "\n",
    "    for _, entry in tqdm(prompts_df.iterrows(), total=len(prompts_df)):\n",
    "        image_file = os.path.join(dataset_dir, entry[\"image\"])\n",
    "        output_image = process_single_image(image_file, entry[\"prompt\"], pipeline_model)\n",
    "        output_image.save(os.path.join(output_dir, entry[\"image\"]))\n",
    "\n",
    "    print(\"Completed!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "PROMPTS_FILE = 'demo_dataset_prompts.tsv'\n",
    "DATASET_DIRECTORY = 'demo_dataset/'\n",
    "OUTPUT_DIRECTORY = 'outputs_submit_multi_check2/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'dropout': 0.0, 'sample_size': 32} were passed to ControlNetModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d57bc43af2f74b5480f1fa3e2e258675"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a4246fa5a584dd39eecd596f212949b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8576ea64492b413d8e598f1d336672bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Bauhaus-inspired living room with a sleek black leather sofa, a tubular steel coffee table exemplifying modernist design, and a geometric patterned rug adding a touch of artistic flair.\n",
      "(1344, 896) (768, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96c1230058674f9886a880e00b54b9ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/45 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cc7d947843143929865b4f0ce4be65b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A glamorous master bedroom in Hollywood Regency style, boasting a plush tufted headboard, mirrored furniture reflecting elegance, luxurious fabrics in rich textures, and opulent gold accents for a touch of luxury.\n",
      "(1344, 896) (768, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0400b5d679d642d0908e00a5b5698360"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/45 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b15252a8cb694bb1af7efab9b5c7d4b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vibrant living room with a tropical theme, complete with comfortable rattan furniture, large leafy plants bringing the outdoors in, bright cushions adding pops of color, and bamboo blinds for natural light control.\n",
      "(1344, 896) (768, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "401a2036796d42fbada881fbe3c90864"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/45 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "460226841cea4835a99ee00a76e2d3f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "execute_pipeline(PROMPTS_FILE, DATASET_DIRECTORY, OUTPUT_DIRECTORY)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
